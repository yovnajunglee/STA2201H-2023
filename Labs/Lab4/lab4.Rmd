---
title: "Week 5: Bayesian linear regression and introduction to Stan"
date: today
date-format: "DD/MM/YY"
format: pdf
execute: 
  warning: false
  message: false
---

```{r}
library(tidyverse)
library(rstan)
library(tidybayes)
library(here)
library(broom)
theme_set(theme_bw())
```


```{r}
kidiq <- read_rds(("kidiq.RDS"))
kidiq
```


## Question 1

Use plots or tables to show three interesting observations about the data. Remember:

- Explain what your graph/ tables show
- Choose a graph type that's appropriate to the data type


```{r}
#colnames(kidiq)
kidiq |> ggplot(aes(x = factor(mom_hs), y = kid_score)) +
  geom_boxplot() + labs(x = "Mom education")
kidiq |> ggplot(aes(x = mom_iq, y = kid_score)) + 
  geom_point()
kidiq |> ggplot(aes(x = mom_age, y = kid_score)) +
  geom_point() + geom_smooth()

```
1. From the box plot, we note that the median IQ score is higher when mom possesses high-school education level. However the distributions overlap. 
2. From the first scatter plot, we note that as the mom's IQ score increases, the child's score also increases i.e. there is a positive relationship between those variables.
3. The second scatter plot does not show any relationship between the mom's age and the child's IQ score.


```{r, results='hide'}

# Fit first model with uninformative prior

y <- kidiq$kid_score
mu0 <- 80
sigma0 <- 10

# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = sigma0)
fit <- stan(file = ("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500)
```
```{r}
fit
```

## Question 2

Change the prior to be much more informative (by changing the standard deviation to be 0.1). Rerun the model. Do the estimates change? Plot the prior and posterior densities. 



```{r, results='hide'}
# named list to input for stan function
data <- list(y = y, 
             N = length(y), 
             mu0 = mu0,
             sigma0 = 0.1)

fit.informative <- stan(file = ("kids2.stan"),
            data = data,
            chains = 3,
            iter = 500)

```


```{r}
fit.informative

post = extract(fit.informative)
post_samples_fit = extract(fit)
```

The estimate for the mean parameter decreases with the informative prior and the variance increases slightly. The standard deviation with the informative prior is much lower. This can be shown in the plots below where the densities of the posterior samples are flatter with the uninformative prior. 


```{r}

# Posterior densities

plot(density(post[["mu"]]), xlim = c(79,91), 
     main = "", col = 'red')
lines(density(post_samples_fit[["mu"]]))
legend("topright", bty = 'n', col = c('red', 1), 
       legend = c("Informative prior", "Uninformative prior"), 
       lty =1)


plot(density(post[["sigma"]]), xlim = c(18,25), 
     main ="", ylim = c(0, .7), col = 'red')
lines(density(post_samples_fit[["sigma"]]))
legend("topright", bty = 'n', col = c('red', 1), 
       legend = c("Informative prior", "Uninformative prior"), 
       lty =1)


# Prior densities

plot(dnorm(seq(mu0-10,mu0 + 10, by =.1), 
           mean = mu0, sd = 10), type ='l',
     ylab = "Density of uninformative prior", xlab = "x")
plot(dnorm(seq(mu0-10,mu0 + 10, by =.1),
            mean = mu0, sd = 0.1), type ='l',
     ylab = "Density of informative prior", xlab = "x")

```


```{r, results='hide'}
X <- as.matrix(kidiq$mom_hs, ncol = 1) # force this to be a matrix
K <- 1

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit2 <- stan(file = ("kids3.stan"),
            data = data, 
            iter = 1000)
```

## Question 3

a) Confirm that the estimates of the intercept and slope are comparable to results from `lm()` 


```{r}
m1 <- lm(kid_score ~ factor(mom_hs), kidiq)
summary(m1)
fit2
```

The estimates of the Bayesian model are very similar to the MLE model.

b) Do a `pairs` plot to investigate the joint sample distributions of the slope and intercept. Comment briefly on what you see. Is this potentially a problem?

```{r}
pairs(fit2, pars = c("alpha", "beta[1]"))
```

We note that the posterior samples between the parameters are highly correlated which indicates that the sampler is inefficient since we note that high slope values lead to low intercept values.


## Question 4

Add in mother's IQ as a covariate and rerun the model. Please  mean center the covariate before putting it into the model. Interpret the coefficient on the (centered) mum's IQ. 

```{r, results='hide'}
X <- as.matrix(cbind(kidiq$mom_hs, 
                     kidiq$mom_iq-mean(kidiq$mom_iq)),
               ncol = 1) # force this to be a matrix
K <- 2

data <- list(y = y, N = length(y), 
             X =X, K = K)
fit3 <- stan(file = ("kids3.stan"),
            data = data, 
            iter = 1000)
```

```{r}
fit3
```
It is expected that as the centered mom's IQ score increases by 1 unit, the child's IQ score increases by 0.56 units.

## Question 5 

Confirm the results from Stan agree with `lm()`

```{r}
m2 <- lm(kid_score ~ factor(mom_hs) + I(mom_iq-mean(mom_iq)), kidiq)
summary(m2)
```

The results are similar. 

## Question 6

Plot the posterior estimates of scores by education of mother for mothers who have an IQ of 110. 

```{r, fig.height=10}
m <- mean(kidiq$mom_iq)
post_samples = extract(fit3)

par(mfrow = c(2,1))
post_y1 <- (post_samples[["alpha"]] +
              post_samples[["beta"]][,1] + (110 - m)*post_samples[["beta"]][,2])
hist(post_y1,  freq = F, xlim = 
       c(80, 100), 
     main = "With high school education", 
     xlab  = "score")

post_y0 <- (post_samples[["alpha"]] +
              0*post_samples[["beta"]][,1] + (110 - m)*post_samples[["beta"]][,2])
hist(post_y0 , freq = F,  
     xlim = c(80, 100), 
     main = "Without high school education", 
     xlab  = "score")
```

## Question 7

Generate and plot (as a histogram) samples from the posterior predictive distribution for a new kid with a mother who graduated high school and has an IQ of 95. 


```{r}
# Sample from posterior predictive distribution

ynew <- rnorm(n = 1000)*(post_samples[["sigma"]]) + 
  post_samples[["alpha"]] + 
  1*post_samples[["beta"]][,1] + 
  (95 - m)*post_samples[["beta"]][,2]
hist(ynew, freq = F)

```

